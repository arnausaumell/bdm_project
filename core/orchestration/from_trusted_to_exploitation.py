import os
import sys

sys.path.append(os.getcwd())

import math
from loguru import logger
from prefect import flow, task
from prefect.cache_policies import NO_CACHE
from dotenv import load_dotenv
from pyspark.sql import DataFrame
from pyspark.sql.functions import col
from core.orchestration.utils import test_environment
from core.landing_and_trusted_zone.deltalake_manager import DeltaLakeManager
from core.exploitation_zone.supabase_client import supabase_client
from core.exploitation_zone.pinecone_manager import PineconeManager

load_dotenv()


def to_pandas_for_upsert(df: DataFrame) -> list[dict]:
    for name, dtype in df.dtypes:
        if dtype == "timestamp":
            df = df.withColumn(name, col(name).cast("string"))
    records = df.toPandas().to_dict(orient="records")
    return [
        {
            k: v if not (isinstance(v, float) and math.isnan(v)) else None
            for k, v in row.items()
        }
        for row in records
    ]


@task(cache_policy=NO_CACHE)
def upload_movies_to_supabase(
    source_manager: DeltaLakeManager,
) -> list[dict]:
    """
    Read movies data from trusted zone and upload to Supabase.

    Args:
        source_manager: DeltaLakeManager for reading from trusted zone
    """
    try:
        logger.info("Uploading movies data to Supabase")

        # Upsert movies
        movies_df = source_manager.read_table("movies")
        movies_list = to_pandas_for_upsert(movies_df)
        logger.info(f"Uploading {len(movies_list)} movies to Supabase")

        # Upsert cast
        cast_df = source_manager.read_table("cast")
        cast_list = to_pandas_for_upsert(cast_df)
        logger.info(f"Uploading {len(cast_list)} cast to Supabase")

        # Upsert movies_cast
        movies_cast_df = source_manager.read_table("movies_cast")
        movies_cast_list = to_pandas_for_upsert(movies_cast_df)
        logger.info(f"Uploading {len(movies_cast_list)} movies_cast to Supabase")

        # Use upsert to handle existing records
        _ = (
            supabase_client.table("movies")
            .upsert(movies_list, on_conflict="tmdb_id")
            .execute()
        )
        try:
            _ = (
                supabase_client.table("cast")
                .upsert(cast_list, on_conflict="cast_id")
                .execute()
            )
        except Exception as e:
            logger.error(f"ERROR uploading cast: {e}")
        try:
            movies_cast_list = [
                {
                    "tmdb_id": row["tmdb_id"],
                    "cast_id": row["cast_id"],
                    "character": row["character"],
                    "role": row["role"],
                }
                for row in movies_cast_list
            ]
            _ = supabase_client.table("movies_cast").upsert(movies_cast_list).execute()
        except Exception as e:
            logger.error(f"ERROR uploading movies_cast: {e}")
        return movies_list

    except Exception as e:
        logger.error(f"Error in upload_movies_to_supabase: {e}")
        raise


@task(cache_policy=NO_CACHE)
def upload_reviews_to_supabase(
    source_manager: DeltaLakeManager,
) -> None:
    """
    Read reviews data from trusted zone and upload to Supabase.

    Args:
        source_manager: DeltaLakeManager for reading from trusted zone
    """
    try:
        logger.info("Uploading reviews data to Supabase")

        # Upsert reviews
        reviews_df = source_manager.read_table("reviews")

        reviews_list = to_pandas_for_upsert(reviews_df)
        logger.info(f"Uploading {len(reviews_list)} reviews to Supabase")

        # Use upsert to handle existing records
        _ = supabase_client.table("reviews").upsert(reviews_list).execute()

    except Exception as e:
        logger.error(f"Error in upload_reviews_to_supabase: {e}")
        raise


@task(cache_policy=NO_CACHE)
def upload_genres_to_supabase(
    source_manager: DeltaLakeManager,
) -> None:
    """
    Read genres data from trusted zone and upload to Supabase.

    Args:
        source_manager: DeltaLakeManager for reading from trusted zone
    """
    try:
        logger.info("Uploading genres data to Supabase")

        # Upsert genres
        genres_df = source_manager.read_table("genres")
        genres_list = to_pandas_for_upsert(genres_df)
        logger.info(f"Uploading {len(genres_list)} genres to Supabase")

        # Upsert movies_genre
        movies_genres_df = source_manager.read_table("movies_genres")
        movies_genres_list = to_pandas_for_upsert(movies_genres_df)
        movies_genres_list = [
            {
                "tmdb_id": row["tmdb_id"],
                "genre_id": row["genre_id"],
            }
            for row in movies_genres_list
        ]
        logger.info(f"Uploading {len(movies_genres_list)} movies_genres to Supabase")

        # Use upsert to handle existing records
        _ = (
            supabase_client.table("genres")
            .upsert(genres_list, on_conflict="genre_id")
            .execute()
        )
        _ = supabase_client.table("movies_genres").upsert(movies_genres_list).execute()

    except Exception as e:
        logger.error(f"Error in upload_genres_to_supabase: {e}")
        raise


@task(cache_policy=NO_CACHE)
def upload_languages_to_supabase(
    source_manager: DeltaLakeManager,
) -> None:
    """
    Read languages data from trusted zone and upload to Supabase.

    Args:
        source_manager: DeltaLakeManager for reading from trusted zone
    """
    try:
        logger.info("Uploading languages data to Supabase")

        # Upsert languages
        languages_df = source_manager.read_table("languages")
        languages_list = to_pandas_for_upsert(languages_df)
        logger.info(f"Uploading {len(languages_list)} languages to Supabase")

        # Upsert movies_language
        movies_languages_df = source_manager.read_table("movies_languages")
        movies_languages_list = to_pandas_for_upsert(movies_languages_df)
        movies_languages_list = [
            {
                "tmdb_id": row["tmdb_id"],
                "iso_639_1": row["iso_639_1"],
            }
            for row in movies_languages_list
        ]
        logger.info(
            f"Uploading {len(movies_languages_list)} movies_languages to Supabase"
        )

        # Use upsert to handle existing records
        _ = (
            supabase_client.table("languages")
            .upsert(languages_list, on_conflict="iso_639_1")
            .execute()
        )
        _ = (
            supabase_client.table("movies_languages")
            .upsert(movies_languages_list)
            .execute()
        )

    except Exception as e:
        logger.error(f"Error in upload_languages_to_supabase: {e}")
        raise


@task(cache_policy=NO_CACHE)
def upload_keywords_to_supabase(
    source_manager: DeltaLakeManager,
) -> None:
    """
    Read keywords data from trusted zone and upload to Supabase.

    Args:
        source_manager: DeltaLakeManager for reading from trusted zone
    """
    try:
        logger.info("Uploading keywords data to Supabase")

        # Upsert keywords
        keywords_df = source_manager.read_table("keywords")
        keywords_list = to_pandas_for_upsert(keywords_df)
        logger.info(f"Uploading {len(keywords_list)} keywords to Supabase")

        # Upsert movies_keywords
        movies_keywords_df = source_manager.read_table("movies_keywords")
        movies_keywords_list = to_pandas_for_upsert(movies_keywords_df)
        movies_keywords_list = [
            {
                "tmdb_id": row["tmdb_id"],
                "keyword_id": row["keyword_id"],
            }
            for row in movies_keywords_list
        ]
        logger.info(
            f"Uploading {len(movies_keywords_list)} movies_keywords to Supabase"
        )

        # Use upsert to handle existing records
        _ = (
            supabase_client.table("keywords")
            .upsert(keywords_list, on_conflict="keyword_id")
            .execute()
        )
        _ = (
            supabase_client.table("movies_keywords")
            .upsert(movies_keywords_list)
            .execute()
        )

    except Exception as e:
        logger.error(f"Error in upload_keywords_to_supabase: {e}")
        raise


@task(cache_policy=NO_CACHE)
def upload_providers_to_supabase(
    source_manager: DeltaLakeManager,
) -> None:
    """
    Read providers data from trusted zone and upload to Supabase.

    Args:
        source_manager: DeltaLakeManager for reading from trusted zone
    """
    try:
        logger.info("Uploading providers data to Supabase")

        # Upsert providers
        providers_df = source_manager.read_table("providers")
        providers_list = to_pandas_for_upsert(providers_df)
        logger.info(f"Uploading {len(providers_list)} providers to Supabase")

        # Upsert movie_providers
        movies_providers_df = source_manager.read_table("movies_providers")
        movies_providers_list = to_pandas_for_upsert(movies_providers_df)
        movies_providers_list = [
            {
                "tmdb_id": row["tmdb_id"],
                "provider_id": row["provider_id"],
                "provider_type": row["provider_type"],
            }
            for row in movies_providers_list
        ]
        logger.info(
            f"Uploading {len(movies_providers_list)} movies_providers to Supabase"
        )

        # Use upsert to handle existing records
        _ = (
            supabase_client.table("providers")
            .upsert(providers_list, on_conflict="provider_id")
            .execute()
        )
        _ = (
            supabase_client.table("movies_providers")
            .upsert(movies_providers_list)
            .execute()
        )

    except Exception as e:
        logger.error(f"Error in upload_providers_to_supabase: {e}")
        raise


@task(cache_policy=NO_CACHE)
def upload_imdb_data_to_supabase(
    source_manager: DeltaLakeManager,
) -> None:
    """
    Read IMDB data from trusted zone and upload to Supabase.
    Includes tmdb_id for easier cross-referencing.

    Args:
        source_manager: DeltaLakeManager for reading from trusted zone
    """
    try:
        logger.info("Uploading IMDB data to Supabase")

        # Read imdb_data
        imdb_data_df = source_manager.read_table("imdb_data")

        # Read movie_ids_and_trailer to get the mapping between imdb_id and tmdb_id
        movie_ids_and_trailer_df = source_manager.read_table("movie_ids_and_trailer")

        # Join the dataframes to include tmdb_id in the imdb_data
        joined_df = imdb_data_df.join(
            movie_ids_and_trailer_df.select("imdb_id", "tmdb_id", "trailer"),
            on="imdb_id",
            how="left",
        )

        # Convert to pandas and then to a list of dictionaries
        imdb_data_list = to_pandas_for_upsert(joined_df)
        logger.info(f"Uploading {len(imdb_data_list)} IMDB data records to Supabase")

        # Use upsert to handle existing records
        _ = (
            supabase_client.table("imdb_data")
            .upsert(imdb_data_list, on_conflict="imdb_id")
            .execute()
        )

    except Exception as e:
        logger.error(f"Error in upload_imdb_data_to_supabase: {e}")
        raise


@task(cache_policy=NO_CACHE)
def upload_external_ratings_to_supabase(
    source_manager: DeltaLakeManager,
) -> None:
    """
    Read external ratings data from trusted zone and upload to Supabase.
    Includes tmdb_id for easier cross-referencing.

    Args:
        source_manager: DeltaLakeManager for reading from trusted zone
    """
    try:
        logger.info("Uploading external ratings data to Supabase")

        # Read external_ratings
        external_ratings_df = source_manager.read_table("external_ratings")

        # Read movie_ids_and_trailer to get the mapping between imdb_id and tmdb_id
        movie_ids_and_trailer_df = source_manager.read_table("movie_ids_and_trailer")

        # Join the dataframes to include tmdb_id in the external_ratings
        joined_df = external_ratings_df.join(
            movie_ids_and_trailer_df.select("imdb_id", "tmdb_id"),
            on="imdb_id",
            how="left",
        )

        # Convert to pandas and then to a list of dictionaries
        external_ratings_list = to_pandas_for_upsert(joined_df)
        external_ratings_list = [
            {
                "tmdb_id": row["tmdb_id"],
                "source": row["source"],
                "value": row["value"],
            }
            for row in external_ratings_list
        ]
        logger.info(
            f"Uploading {len(external_ratings_list)} external ratings records to Supabase"
        )

        # Use upsert to handle existing records
        _ = (
            supabase_client.table("external_ratings")
            .upsert(external_ratings_list)
            .execute()
        )

    except Exception as e:
        logger.error(f"Error in upload_external_ratings_to_supabase: {e}")
        raise


@task(cache_policy=NO_CACHE)
def upload_youtube_data_to_supabase(
    source_manager: DeltaLakeManager,
) -> None:
    """
    Read YouTube data from trusted zone and upload to Supabase.
    Includes both statistics and comments.

    Args:
        source_manager: DeltaLakeManager for reading from trusted zone
    """
    try:
        logger.info("Uploading YouTube data to Supabase")

        # Upsert youtube_statistics
        try:
            youtube_statistics_df = source_manager.read_table("youtube_statistics")
            youtube_statistics_list = to_pandas_for_upsert(youtube_statistics_df)
            logger.info(
                f"Uploading {len(youtube_statistics_list)} YouTube statistics records to Supabase"
            )
            _ = (
                supabase_client.table("youtube_statistics")
                .upsert(youtube_statistics_list, on_conflict="tmdb_id")
                .execute()
            )
        except Exception as e:
            logger.error(f"Error uploading YouTube statistics: {e}")

        # Upsert youtube_comments
        try:
            youtube_comments_df = source_manager.read_table("youtube_comments")
            youtube_comments_list = to_pandas_for_upsert(youtube_comments_df)
            logger.info(
                f"Uploading {len(youtube_comments_list)} YouTube comments to Supabase"
            )
            _ = (
                supabase_client.table("youtube_comments")
                .upsert(youtube_comments_list)
                .execute()
            )
        except Exception as e:
            logger.error(f"Error uploading YouTube comments: {e}")

    except Exception as e:
        logger.error(f"Error in upload_youtube_data_to_supabase: {e}")
        raise


def format_movies_text(movie: dict) -> str:
    print(f"{movie['title']} - {movie['overview']}")
    return f"{movie['title']} - {movie['overview']}"


@flow(name="from-trusted-to-exploitation")
def trusted_to_exploitation() -> None:
    """
    Main function to transfer data from trusted zone to exploitation zone (Supabase).
    """
    try:
        logger.info("Starting trusted_to_exploitation flow")

        # Test environment first
        test_environment()

        # Initialize DeltaLakeManager for source
        logger.info("Initializing DeltaLakeManager")
        source_manager = DeltaLakeManager(
            s3_bucket_name="bdm-movies-db-trusted",
        )

        # Upload each table to Supabase
        movies_list = upload_movies_to_supabase(source_manager)
        upload_reviews_to_supabase(source_manager)
        upload_genres_to_supabase(source_manager)
        upload_languages_to_supabase(source_manager)
        upload_keywords_to_supabase(source_manager)
        upload_providers_to_supabase(source_manager)
        upload_imdb_data_to_supabase(source_manager)
        upload_external_ratings_to_supabase(source_manager)
        upload_youtube_data_to_supabase(source_manager)

        # Upsert to Pinecone
        pinecone_manager = PineconeManager(index_name="bdm-movies")
        pinecone_manager.upsert_items(
            [
                {
                    "id": str(movie["tmdb_id"]),
                    "text": format_movies_text(movie),
                    "metadata": movie,
                }
                for movie in movies_list
            ]
        )

        logger.info("Data transfer to exploitation zone completed successfully")

    except Exception as e:
        logger.error(f"Error in trusted_to_exploitation: {e}")
        raise


if __name__ == "__main__":
    from prefect.docker import DockerImage

    logger.info("Creating deployment")
    deployment = trusted_to_exploitation.deploy(
        name="from-trusted-to-exploitation",
        work_pool_name="my-docker-pool",
        cron="0 2 * * *",  # Run at 2 AM daily
        image=DockerImage(
            name="arnausau11/orchestration",
            tag="latest",
            dockerfile="core/orchestration/Dockerfile",
        ),
        push=True,
    )
    logger.info("Deployment created")
